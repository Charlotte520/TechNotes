1. 数据一致性：
1) gossip
一传十，十传百

redis, cassandra, amazon s3, bit torrent
问题：如何分布式数据达到一致状态？ 
简单方案：数据先存到server x，其他a、b、c等定期到x上pull。 =》x挂掉怎么办？ 由于网络原因，a连不上x怎么办？
gossip：x随机选#fanout个相邻节点发消息，如给a、b、c、d。a-d再分别随机选#f个相邻节点继续转发。经过log(#node)，base #fanout次，所有节点一致。也可以pull，或push-pull结合。
优点：传播快，可伸缩，多路径=》容错性强，去中心化。
缺点：延迟；冗余。

2) raft：类paxos
如何选leader？每个节点都有一个timer，初始为0。分配一个随机的election timeout（等待时间，150-300ms之间的随机值），结束后发起竞选。
如a(10ms),b(15ms),c(20ms)，a先发起竞选，增加自己的term，给自己投票，发请求给b、c。若bc还没有对该term投过票，bc重置自己的timer为0，同意a的请求。当a投票数过半后成为leader，发append entries消息给bc，维持与bc的心跳。bc每次收到心跳，timer都重置为0.
a挂掉，若bc同时发起竞选，每个节点投票计数都为1：重置随机等待时间，重试。
如何复制数据？log replication也有append entries。每个节点都有一个日志队列。client发写请求到a，a记录到log（未提交）。a将数据发给bc，bc记录到log（未提交），告诉a已经记录。a收到半数响应，提交数据，响应client，并通知bc已提交。bc提交数据。若b失联，a不断重试，若重新开始工作，从a复制数据。

a commit，挂掉，其他节点没有commit，怎么办？独立线程补偿。 a 挂掉，还没选出新的leader，client请求怎么办？重试。

3)paxos：超半数投票，只用最新投票
选leader：先选一个投票负责人，收集所有机器的投票，排序，将票数最高的选做leader。=》投票负责人单点 =》机器间以p2p方式，互相发消息选leader，保证有>n/2台机器可用就可以完成投票，无单点。
申请阶段：先在25台机器中找到5台选举队长acceptor，跟所有机器通信确定leader candidate（3个）。20台机器proposer给5个acceptor都发epoch，队长判断若epoch最新，返回ok。
投票阶段：若某proposer p收到超一半acceptor的ok（3），告诉返回ok的队长要投票的candidate c。若3个acceptor没有其他更新的epoch，且保持与该proposer的通信，都给他返回suc。p收到3个suc，则确定leader为c。再有其他propeser申请投票，则acceptor直接返回c。
若投票不顺利，5个accptor，2个投c，1个投b，1个投d，无法确定结果。p收到2个c，1个b，看哪个更新，若为b，改投b，再发给acceptor，此时有3个b。经过很久后达到一致。

2.zk
树形结构数据可在多机器间可靠复制，达到一致性。
session：client和zk建立连接，定期发心跳，超时收不到则结束session。
znode：树中每个节点，有永久（主动删除）、临时（session结束删除）、顺序（可实现分布式锁）。
watch：client可监听znode，有变化（delete、update）则收到通知。

3. cap
网络节点无法通信时，数据复制相关的功能，要么选A（能提供但可能不一致），要么选C（不可用，但其他功能可以访问）。
一致性：数据冗余导致不一致
session：多web server间同步（占带宽，有延迟，内存有限）；存在client（每次http请求都要带session，占外网带宽，有安全隐患，大小受cookie限制）；nginx根据用户ip（4层）/业务属性（7层，uid）做hash（server重启会丢部分session，要重新登录，rehash要重分布）；后端存储到redis/db。
db主从：读多写少，读写分离，冗余从库。主从延迟导致读旧数据：半同步（主从同步完再返回写成功，写延迟增大）；强制读主库（用cache提高读性能）；中间件（记录写的key，在时间窗内将对key的读请求路由到主库，其他的路由到从库）；将写的key记录到cache，设置超时时间，读时若key在cache读主库，否则从库。
db双主：双向同步，冗余写库提高写性能。自增主键冲突导致同步失败：两库不同的初始值，相同的步长。全局id gen。shadow master：只有一个主库可写，两库有相同vip，主库挂掉时vip自动漂移到shadow。若先写到主1，挂掉，vip到主2,写丢失。=》监测器：定时监测主1是否可用，异常时delay x秒，等主2同步完再切换到主2。
db主从与cache：写数据时，更新or淘汰cache？若计算复杂，更新代价高，直接淘汰，否则更新（减少miss）。先操作db or cache？先淘汰cache，再写db（有cache miss，但可避免不一致）。写数据时，先淘汰cache，尚未更新db时，有其他读请求将旧数据又读到cache；写到主库，但尚未同步到从库时，从库读旧数据读到cache。导致cache和db不一致，要cache淘汰两次：写db后，等1s（同步延时），再次淘汰cache。或根据binlog，异步第二次淘汰cache。或对同一id的数据访问串行化，hash(id)%#conn。
db冗余：水平切分时，根据partitionkey可定位到库，但非partitionkey的查询要扫全库，用冗余表提高非partitionkey的查询效率。服务可同步双写两表（写慢）；异步双写（写表1，发到queue，或根据log，异步服务写入表2）。都有不一致问题：离线扫描工具，对比两表，补偿不一致。或根据log，增量扫描。或写db时，消息发到queue，订阅该msg的服务实时检测并补偿。
消息时序：单聊（发送方发送顺序与接收方展示顺序一致），群聊（所有接收方展示顺序一致），支付（同一用户的请求在server执行顺序一致）。原因：分布式时钟不一致、多发送方、多接收方、网络传输、多线程。=》以client/server时序为准。server生成单调递增id。主从复制时,在主上单点序列化，再将操作序列发到所有从。单聊：发消息时加上本地时序。群聊：server生成全局递增id；同一群的消息发到同一server，局部递增id。
事务：2pc：prepare+commit，并行执行，统一提交（降低不一致的概率，conn占用时间长）。协调者+参与者。投票阶段：参与者通知协调者，参与者回复ok前阻塞内部资源。提交阶段：协调者根据执行反馈发commit/rollback请求。缺：阻塞后协调者挂，资源一直不释放。提交阶段挂，不一致。阻塞范围过大。3pc：prepare+precommit（阻塞）+commit。增加复杂度，性能降低。解决了协调者单点问题：新协调者根据precommit的结果。用timeout避免永久阻塞。tcc：本地事务代替全局事务，不需要协调者。用事务log保证故障恢复后的一致性。都要求幂等。
补偿事务，业务层逆向操作db（if/else多，补偿可能失败）。显示回滚：调用逆向接口（确定失败的步骤和状态，回滚范围）；隐式回滚，不掉逆向接口（下单时会先预占库存，15min后未支付直接释放库存）。
重试：若下游返回超时、限流中等临时状态，可重试；若返回无权限、余额不足等明确错误，不重试；返回503、404等不知何时恢复的错误，不重试。策略：立即重试（暂时性故障，网络包冲突、流量高峰等），不超过1次；固定间隔，适合前端交互系统；增量间隔，降低失败次数多的请求的优先级；指数间隔；随机间隔。终止策略。幂等：识别是否重复请求，如请求id，判断该请求是否已被执行。


不同级别的一致性：
1)最终一致性：先保证局部一致，在未来某时刻达到顺序一致。
因果前后一致；读你所写一致（朋友圈回复一句话，对方不能立即看到，但自己必须能看到）；会话一致（逻辑先后）
2)顺序一致性：保证所以进程以相同顺序看到所有的共享访问。与时间无关。
3)线性化：顺序一致基础上，保证在全局的相对时间下顺序一致。
4)绝对一致：所有共享访问按绝对时间排序，理论。

拜占庭将军问题：
拜占庭错误（通过伪造信息进行恶意响应产生的错误） BFT（Byzantine Fault Tolerance）类算法；非拜占庭错误（没有响应产生的错误，消息可能丢失/重复，但无错误消息） CFT（Crash Fault Tolerance）类算法，如raft =》如何使某变更在分布式网络中得到一致的执行结果，被多参与方承认，且不可推翻。
BFT类算法：基于确定性的（达成共识后不可逆转，如PBFT（Practical Byzantine Fault Tolerance），最多容忍1/3失效节点）；基于概率的（共识结果是临时的，但随着时间推移或某种强化，被推翻的概率越来越小，如POW，最多1/2失效节点）。
CFT类算法：paxos（每次变更都有唯一id，且能识别新旧；进程只能接受比已知id更新的id；任意两次变更必须有相同的进程参与），无leader算法。raft：有leader算法。只有一个alive leader，负责与follower同步；若leader失联，每个follower都可能成为leader，term最新的成功。follower的投票先到先得，但有可能多个term相同的候选人得到相同的票数，即分割投票问题。则再开始一轮投票，直到成功。且用随机定时器自增term，减少相同票数的概率。zab：基于fast paxos。

可用性：99.9%，年停机8.76h，99.99%为52.56min  冗余/去单点+故障自动转移
健康探测：http（get/post某固定url，根据http response code、内容判断）；tcp（ip、port）；udp（若server无返回则正常，否则返回icmp unreachable错误）

高并发：响应时间、吞吐量、qps、并发用户数等指标。scale up：硬件；架构（cache、异步化、无锁数据结构）。scale out。


base：
basically available：系统故障时，允许损失部分可用功能，保证核心功能。
soft sate：状态可有一段时间不同步，且该状态不影响系统可用性。
eventually consistent。

4. GFS，MapReduce、BigTable
GFS：单点master（避免分布式锁），shadow-master，只存meta不存文件数据，不走数据流，client可缓存meta。chunk server：64M文件块（减少碎片），追加写，tcp长连接。可靠性：meta变更写log；master检测chunk server存活性；master加锁保证meta的修改是原子的，顺序性；用checksum保证数据正确。保证多chunk server数据一致：primary串行化所有写操作。读多写少，所有写成功才返回，读一个成功即可，W+R>N。
MapReduce：分治。用户设置#M，#R。创建M+R+1个执行实例副本，1master+其他worker。输入数据分为M份，分配给map worker，执行map()，本地内存生成临时数据，用partition()将数据分成R份，周期性写本地磁盘，由master传到reduce worker。reducer从M个map读数据(基于ip尽可能近)，执行reduce()，结果输出。全部结束后，master唤醒用户程序，返回R个结果文件。master：存meta（可存磁盘，用shadow-master做高可用；master挂掉重新执行），监控状态。worker：master周期ping，超时重新执行其job。若重执行map，要通知reducer新的mapper。若重执行reducer，不用通知。要求job幂等。长尾worker：partition()不合理，导致reduce负载不均。worker机器磁盘、cpu等有问题：备用worker，某些worker执行时间超预期，启动另一个worker执行相同job。
分层架构：cpu固定，数据移动。下层封装获取数据的细节，上层更高效地复用数据。MR：数据量大，固定数据，移动cpu。
BigTable:三维，key+column+time => value。

高可用三剑客：降级、限流、熔断
5.降级
有限资源效益最大化。
牺牲用户体验：减少冷数据获取（禁用list翻页）；放缓流量进入速率（加验证码）；减少大查询占用的资源，提高筛选条件要求（禁用模糊查询、部分条件必选）；静态数据，甚至直接显示xx功能暂时关闭。
牺牲功能完整性：临时关闭风控、取消部分条件判断（积分商品添加时判断积分够不够）；关闭info、warn log。
牺牲时效性：商品页不显示库存数，仅显示是否有货；异步操作速率放缓（如送积分、券等）

1)定级定序：确定每个功能重要度级别；再根据每个程序所支撑的上游功能个数、功能重要度等指标，确定顺序。被依赖的下游不能先于上游被降级。
2)实现：触发机制（超时率、错误率、资源消耗率等）。
int runlevel = 3;
int runindex = 7;
if (mylevel>runlevel && myindex>runindex) 降级 //用AOP+注解实现
else 正常运行

位置：前端：http response中通过cache-control设置，使后续请求直接走浏览器cache；不加载异步加载的数据；禁用部分操作；静态页面。后端：读操作，无返回值的直接return/throw exp，有返回值的返回mock数据/throw exp。写操作，写db通过mq异步化


6.限流
场景：系统未宕机，但资源不足以满足大量请求，为提供最大化服务能力，限制流量到系统处理能力的上限附近。
1)压测系统上限：独立环境；或在prod选一个节点，并与其他节点隔离。=》单个请求的处理速度；并发数。avg、max、median。
2)干预流量的策略：固定窗口（固定统计周期内，1min，用累加器统计总请求数，达到阈值触发，下个周期counter清零。缺：流量速度通常有波动，counter可能提前计满，周期内剩余时间请求都被限制，或不满，导致资源无法充分利用。临时应急方案）；滑动窗口（1min固窗切分为60个1s的滑窗。仍基于预定义的时间片。对异常结果高容忍场景）；漏桶（固定出口速率，涌入的流量缓冲起来，超过桶容量时干预。通过缓冲区将不平滑的流量变平滑，最大化资源利用率，但多进程时进程处理能力会被干扰，常用较低qps定义出口速率，防止超负荷。通用，宽进严出）；令牌桶（固定入口速率，先拿token再请求，拿不到的被干预。桶容量为最大并发数。适合需要压榨程序性能，流量波动不大的场景）。
3)处理被干预掉的流量：丢弃；暂存后续处理。

位置：接入层，nignx用ngx_http_limit_conn_module和ngx_http_limit_req_module。应用层，用AOP，根据成本针对性限流，如ToC优先于ToB，高频优先于低频，web层filter比service层更容易。应用间限流，client效果好，限流时可不用建立连接，也能分散压力，但成本高，不适合需要多节点互通数据的场景。

固定窗口：
int total=0; //独立线程每隔固定周期清0
if (total>T) return;//拒绝处理
total++;
//处理请求

滑动窗口:
list totals = new list<>(滑窗个数);//独立线程每隔滑窗时间删除totals(0)，并尾部增加新元素
int sum = totals.sum();
if (sum>T) return;
totals.get(last)++;

漏桶:
int outSpeed;//独立线程每隔固定周期清0
int waterlevel;//缓冲区水位
if (outSpeed<T1) {outSpeed++; 处理}
else {
  if (waterlevel>T2) return;
  waterlevel++;
  while(outSpeed>=T1) sleep;//等待
  outSpeed++;
  waterlevel--;
  处理
}

令牌桶:
int tokenCount = T;//独立线程以固定频率增加可用令牌数
if (tokenCount == 0) return;
tokenCount--;
处理

7.熔断 
下游服务因压力大而变慢/失败，上游暂时切断对下游的调用。场景：下游是共享系统，上游仅是其中一个client；下游被部署在共享环境，资源未隔离，可能和其他高负载系统部署在一起；下游常迭代更新；上游流量突增。基于AOP实现。
1)识别下游是否正常：能否调用成功，是否超时。某时间窗口内，x次或x%无法连接或超时大于n秒
2)切断联系：fail fast。
3)探测：每隔段时间再探测（固定，或线性/指数增长）。时间窗口，阈值/百分比。
三种状态：熔断开、关、半开  若流量不稳定，容易频繁切换
errorCount = 0;//单独线程每10秒重置为0
isOpenCircuitBreaker = false;
sucCount = 0;//独立线程每隔时间窗口重置，并将isHalfOpen置为false
isHalfOpen = true;//独立线程每隔 间隔时间，置为true   

if (isOpenCircuitBreaker==true) return defualt/error;
else rpc call;

if (rpc.suc) {
  if (isHalfOpen) {
    sucCount++;
    if (sucCount == 阈值) isOpenCircuitBreaker=false;
  }
  return suc;
} else {
  errorCount++;
  if (errorCount==阈值) isOpenCircuitBreaker=true;
}

8.伸缩性
1)有状态-》无状态：状态通过参数传递(网络包大)；session相关放到入口层，重建成本小，使后端系统无状态；共享存储（cache、db）
2)高内聚低耦合：分层。定职责，提高内聚、复用度。做归类，模块间的依赖关系。划边界，class、function级别通过codereview、静态代码检查；模块级别，打包，不能有逆向依赖，基础jar不能包含其他基础jar；系统级别，通过调用链跟踪分析请求链路是否合法。
3)弹性架构：事件驱动架构，通过队列解耦。缺：一致性，异步延迟。适合实时性要求不高、跨平台异构环境。微内核架构：先实现稳定core，再迭代增加功能，避免单一组件失效使整个系统崩溃。缺：插件间独立，无法复用。
4)分库：按业务。分表：批量查询、分页复杂。先找最高频的读字段，使用特点（批量读/单个读，是否关联其他表字段）。范围切分：单表大小可控，扩展时增加新表即可，不用数据迁移。但有冷热库，新库压力大，适合热点不集中的场景。hash切分：分散压力，二次扩展要迁数据。适合大数据量场景。不停机切分：原库/表打开主从同步，新表为从，进行全量实时同步。再删掉不属于它的数据。
跨库join问题：逻辑在应用层处理。全局聚合/排序问题：多次并行。



