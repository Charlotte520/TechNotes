1.nginx
模块化设计；高可靠性，worker有问题，主进程重启新worker；内存消耗低，1w keep-alive长连接，2.5MB内存；热部署；5w并发。
内核+模块：core收到http请求，查config，将其映射到location block。module处理location中配的各个指令：handler+filter。handler处理请求，生成响应内容。filter处理内容。
从结构上分为3种模块：核心模块：http；event；mail。基础模块：access；fastCGI；proxy；rewrite。第三方模块：upstream request hash；notice；access key。
功能分：handlers：处理请求，输出内容，改header等，一般只有1个。filters：修改其他模块的输出。proxies：upstream类，与后端服务如fastcgi交互，进行代理&LB。
多进程：master+多worker。master与用户交互，管理worker。worker处理client请求，实现重启、平滑升级、换log、reload配置等。创建master时，建立要监听的socket（listenfd）；再fork()多个worker，每个worker分配一个可监听client请求的socket。有connection进来，所有worker都收到通知，只有一个进程接受，惊群现象。通过accept_mutex，获得mutex的进程才添加accept事件，避免惊群。每个worker有独立的connection pool，用free_connections保存所有空闲connection_t结构，获取连接时从free表获取，用完放回。故nginx能建立的最大连接数=#workers* poolsize。若作为反向代理，每个并发要建立与client和server的两条连接，要/2。
http请求流程：建立conn，读取一行数据，得到method、uri、http_version。再逐行处理header，得到是否有body和length。处理body。
热部署：master读取配置更新，不立即通知worker。等worker执行完毕，关闭子进程，创建新进程，执行新配置。

2. kafka
messaging system + stream processing system。支持producer/consumer点对点传输 + pub/sub 多对多模型。保证消息可靠传递；持久化、多副本容错；高吞吐，单broker可处理几千partition、百万/s消息。用于解耦、削峰填谷、异步化。
与rabbitmq对比：rabbitmq是传统消息队列，支持AMQP等协议，更复杂的consumer routing，事务。kafka性能更高。
选举：controller；分区leader；消费者相关。
1. controller：集群中有多个broker，启动时向zk注册，将id写入/broker/ids临时节点。竞争创建/controller节点，只有1个被选为controller，管理所有partition和replica的状态。若broker宕机、zk连接超时，触发watcher。如：若某partition的leader副本出问题，controller为该partition选新leader。若某partition的ISR集合变化，controller通知所有broker更新meta。若controller挂，重选controller。
2. 分区leader：partition创建（创建topic，增加partition） 或 上线（原leader下线，选新leader）时执行。按AR集合中（分配时指定）副本顺序找第一个alive的副本，且在ISR中。分区重分配也要选leader，从新AR表找第一个alive，且在ISR中。当某节点被controlled shutdown时，其上leader副本对应的partition也要重选。producer、consumer只能从partition leader读写数据，follower只做故障转移：partition已经使读请求分散到不同broker，不像mysql主从的压力都在主上；若可以从follower读，consumer offset太复杂，且producer写时也要保证写入所有follower才能返回，写性能低。
3. producer：消息封装为ProducerRecord对象，包括topic、content、key、partition，先将kv序列化为byte[]。根据partitioner.class指定的类/RR选择partition，放入batch record，由异步线程发到broker。broker收到消息，写入成功返回RecordMetaData(topic,partition,offset)，否则返回error，可重试若干次，失败则抛异常。参数：acks=0,发送就认为成功，不用等broker响应；=1,leader收到消息则成功；all，所有isr都成功。buffer.memory。compression.type=snappy，gzip，lz4。retries。batch.size。producer的分区策略：RR，均匀；random，不如RR均匀；相同key发到相同partition；基于geo-location；自定义配置partitioner.class，实现Partitioner接口的partition()。
4. consumer：consumer group用于提高消费吞吐，且某consumer instance挂掉后，将其partition转移到其他consumer，根据zk/kafka中的consumer offset，rebalance，可能导致重复消费/丢消息。
kafka通过特殊topic：ConsumerOffsetTopic，记录consumer offset，cosumer member。以groupid为key。有新consumer加入，向该topic发消息。该topic的consumer将其加入consumer group，分配partition等。也可用来选GroupCoordinator，为组内consumer选一个leader，若第一次选，则为第一个加入group的consumer；否则取hashmap第一个key，随机。
4.broker：用于LB，scalable，高吞吐。数据持久化：append-only log，按时间自动分段，定期删除旧段回收磁盘。只能保证partitin内的消息有序，无法保证全局有序。0 copy：传统磁盘文件发网络需要4次copy（DMA从磁盘到内核buffer，CPU从内核buf到用户buf，CPU从用户buf到内核buf，DMA从内核buf到网卡）。sendfile系统调用可DMA到内核，再到网卡，无需CPU。实现：TransportLayer.tranform()中调用NIO中FileChannel.transferTo()。物理存储：为broker分配partition replica时，尽量使broker的replica均匀，不同replica在不同broker上。数据保留：最长保留时间；最大数据量。分段存储，当前活跃片段不会被删除。

kafka streams：其他streaming框架只能保证框架内的正确性，即exactly-once，无法控制外部数据流转状态。kafka的数据流转、计算都在内部完成，可保证e2e exactly-once。提供的是client lib，非完整功能系统（集群调度、弹性部署等）。

3.redis
基于内存，单线程，非阻塞IO。可作db、缓存、消息服务。支持string、hash、list、set、sorted set、bitmap等数据结构，key为byte[]，最大512MB。可LRU淘汰、事务、不同级别的持久化。通过replica set + sentinel实现高可用。redis cluster数据分片。单进程单线程，用queue将并发变串行。redis无锁，jedis并发访问时，在客户端通过连接池、加锁同步。或用setnx。

基本数据结构：SDS，double linkedlist（有head、tail指针的双向链表），dictionary，skiplist，intset，ziplist。根据使用场景和内容大小，使用不同数据结构，构建对象系统：string、list、hash、set、sorted set。redisObject：type 对象类型, encoding 数据结构, lru, refcount, ptr。 
String：set (ex/px有效期，nx不存在则操作)；getset(set新值并返回原值)，mset（多key,o(n)），msetnx（任一存在则不操作），mget。Incr（能转为整型的数据自增1）,incrby(自增x)，转为64bit long。用动态字符串SDS表示字符串值：len（不包括结束符的实际长度），alloc（最大容量），flags（1B，最低3bit表示header类型），buf（字符数组）。通过预分配和惰性空间，减少修改string时重分配内存次数。若修改后，len<1MB，预分配和2*len空间，若>1MB，预分配len+1MB空间。缩短时不立即释放多的空间。当len>32B，用SDS两次内存分配创建robj和sds，<32B，一次分配连续内存，只读。
list：lpush/rpush/lpop/rpop。在特定index插入，o(n)。blpop，类似blockingqueue，空会阻塞。用ziplist或linkedlist实现。当元素数<512，长度<64B，用ziplist。
hash：用MurmurHash2计算hash，链地址法解决冲突（单向链表）。基于ziplist或dict。当kv<64B,数量<512用ziplist。
集合：基于intset或dict。当元素都为int，且数量<512，用intset。
有序集合：跳表(效率和平衡树差不多，但实现简单)。整数集合：intset，元素为int且个数不多。基于ziplist或skiplist。当元素数<128,且<64B,用ziplist。
压缩队列ziplist：用于list，hash。zlbytes：4B 压缩数组字节数，zltail：4B 尾节点地址，zllen：2B 节点数，zlend：1B 标记末尾。每个entry包括：previous_entry_length，可计算前一节点地址，encoding 当前节点数据类型和长度，content 节点值。

持久化：RDB；AOF。RDB，主进程fork子进程，定期将数据快照保存到rdb文件。不影响client请求效率，恢复快。但可能丢数据。AOF：每个请求写入log，fsync可为always（每写一条log fsync一次，安全但慢），no（os flush），everysec（后台线程每秒一次）。大量无用日志导致log太大，恢复慢。rewirte只保留最小写操作集。
淘汰：达到maxmemory后，根据策略尝试淘汰；若无数据可淘汰或无策略，写请求返回error，读请求正常。主从同步数据时也要用一部分内存，max不能太接近主机可用内存。volatile-lru：LRU，只淘汰设定expire的key。allkeys-lru：lru。volatile-random：随机，expire key。allkeys-random。volatile-ttl：淘汰剩余有效期最短的。推荐：volatile-lru。重要数据（config等），不设有效期。热加载数据设。
过期key的删除策略：定时（每个带expire time的key，创建timer，到时间立即主动删除。省内存，占cpu）。惰性（无timer，get时检查是否exp，若过期则被动删除。省cpu，占内存。）。定期（每隔一段时间检查所有key，主动删除过期的。折中，如何选执行频率和时长？）
pipelining：通过mset/mget等批处理命令减少网络消耗，若连续执行多次无相关性操作，可用pipeline一次请求，server依次执行。若有前后依赖用scripting。
事务：multi开启事务，之后的读写命令放入queue，exec后执行。不支持回滚，可exec时检查queue中命令的语法错误，若有则放弃事务，但无法检查非语法类错误。watch+事务：cas锁。执行exec时，检查被watch的key，若从watch开始至今没有变化，执行exec。
性能：不执行耗时长的命令，o(n)；pipelining连续执行的命令；持久化；读写分离。仅把list当queue；控制hash、set、sorted set大小；禁止keys；避免一次遍历所有成员，用scan游标分批遍历。用长连接/连接池，避免频繁创建/销毁连接；批量操作用pipeline。同一秒有大量key过期会引发redis延迟。
主从复制：一master处理写，其他slave读。master crash，sentinel自动将slave变为master。slave启动时，从master冷启动同步，导入master的rdb，此后master将增量数据同步给slave。
分片：去掉sentinel，由cluster监控分片的节点，自动failover。一致性hash，共16384个hash slot，计算key的crc16 %16384。指定每个partition的slot。hash tag：相同tag的数据放入同一slot，如pipeline、事务涉及的key。缺点：client要为每个partition维护一个connection pool。
client：jedis，支持connection pool，pipeline，事务，sentinel，cluster。不支持读写分离。
哨兵：监测主从库是否正常；主库故障时自动将一个从库切换到主库。若有3个哨兵实例，每个sentinel定期（1s）给master、slave和其他sentinel发ping。若某instance距离最后一次有效回复ping的时间超过down-after-milliseconds，则被标记为主观下线。若master被标记为主观下线，则所有监视他的sentinel要定期（1s）确认其是否进入下线状态。若足够数量sentinel在指定时间范围内确认master为主观下线，则master被标记为客观下线。sentinel向所有slave发info。若没有足够sentinel同意master下线，则移除客观下线状态。

与memchached对比：MM数据全在内存；redis可通过RDB/AOF持久化。redis构建vm，避免系统调用。

4.hdfs
master-slave架构：单namenode + 多datanode。namenode：对namespace操作（create、move、open、close、rename等，ACL，不支持hard/soft link），管理meta（data block位置）。文件目录树，权限设置，副本数设置等。更新内存的文件目录，顺序写editlog，定期checkpoint写回磁盘的fsimage，清空旧log。standby备节点拉取editlog，更新自己的内存，与active保持一致。datanode：可配置block大小、replica数量，默认128MB(减少寻址开销），3。

架构稳定性：1) hb+复制：datanode定期向namenode发heartbeat，超时没有hb的被标记为dead，不再发IO请求。namenode会重新复制block来保证replica。2) 数据完整性：校验。计算每个block的checksum，存在namenode，client读文件时验证datanode数据和namenode checksum是否一致。3)namenode高可用：fsimage + editlog同步。

一次写多次读：简化数据一致性，提高吞吐。不适合低延迟场景（用hbase），大量小文件（每个文件都通过namenode存meta，存储瓶颈），多用户更新（追加写，加锁，用hbase），结构化数据，数据量不大（适合TB、PB级）。

多client同时并发写同一文件：先从namenode获取文件契约，串行化。获取契约的client开启线程发请求到namenode续约，每次写文件时renew。namenode后台线程监听各契约的续约时间，新block选DN，或数据同步到磁盘时，要检查lease。若长时间没有续约则过期，允许其他client写，防止死锁。namenode用treeset根据最后一次续约时间排序，最老的在前（红黑树，无重复），每次检查最旧契约即可，避免遍历所有契约效率低。

replica：机架感知策略，第一个replica在运行client的节点，或随即选第一个节点；与第一个不同，随机选另外机架放第二个；第三个与第二个同机架不同节点。

读：优先读与client最近的replica。client通过open()打开文件，rpc调用namenode，得到block及地址信息。client通过FSDataInputStream.read()读数据，连接保存文件第一个block的最近datanode，读完block后，关闭和此datanode的连接，再连接下一个datanode。读完，FSDataInputStream.close()。若读取时有datanode出错，则尝试连接该block下一datanode，并记录失败node，不再连接。
写：create()创建，在namenode检查是否存在，权限，并创建。client的DFSOutputStream将数据分block，写入data queue。data streamer读取queue，并通知namenode分配datanode，将数据写入pipeline的第一个datanode1，node1发给第二个node2,2发给3。以数据包为单位64KB，每个DN收到包要发ack给client。若client没有收到DN ack，要调整pipeline，不给该DN发数据，replica不足的情况由NN稍后处理。全部写入成功，返回ack给DFSOutputStream。

故障：节点故障；通讯故障；数据故障。NN两张表：block->DN1,DN2,DN3; DN1->block1... 若某DN的block损坏，从block表移除；若某DN挂，更新两表。定期扫描block表，是否每个block都被充分备份，不足的指定DN从其他DN获取备份。

优化：namenode：文件数、block很多导致的内存压力=》将目录树拆分，放到不同NN，将NN和目录树的对应关系放zk；锁竞争（对任一节点加锁要对整个目录树加锁）=》对每个node加锁；跨dc。datanode：机器数上万时的rolling restart（每个dn扫所有磁盘，将所有block上报给NN，NN要加锁更新）=》dn restart前将副本信息存到leveldb，restart后不扫盘，从db load数据。发full block report前先检查NN上次收到block report的时间，有必要才发；机器下线要加锁=》先将DN标记为stale，从NN获取该DN的所有block，将block发到目的DN，再从本DN删除，不需要NN加锁；data lost；slow node。存储效率：降低存储成本。

5. elastic search
如何数十亿数据查询，ms级返回？第一次搜索5-10s，后面ms。
filesystem cache：index尽量放入内存，即机器内存至少可容纳一半数据。数据有30个字段，仅将搜索条件字段写入ES，其他写入hbase。先根据条件在ES找到id，再到hbase拿到完整数据。
数据预热：每隔一段时间，提前访问可能的热点数据，放入fs cache。
冷热分离：根据访问频率设立不同的index，避免热数据cache被冷数据冲掉。
document模型：join、nested、parent-child等复杂的关联查询，在java应用层做，es写关联好的数据。
分页：要查第x页的数据，需要把多个节点的x*10条数据都查到同一节点，再合并、处理得到第x页。翻页越深，每个节点返回的数据越多，协调处理越慢。用scroll，只能连续翻页。

7. mapreduce
split -> map -> map端shuffle：map结果先在mem，达到某阈值后写入磁盘，同时sort、combine、partition。-> reducer shuffle：从多mapper下载数据 -> reducer ->output.
jobtracker：和namenode同机器。tasktracker：和datanode同机器，可将map分给同机器上的tasktracker，将jar包复制到该节点。
client启动job，向jobtracker申请jobid，将资源复制到hdfs（jar、config、划分信息）。jobtracker将其放入queue，等调度器根据算法调度，根据split信息创建map job，分配给tasktracker。根据机器#core、mem大小有固定数量的map、reduce slot。tasktracker定期给jobtracker发heartbeat，包括map进度等信息。map：结果先放入环形mem buffer（100M），达到80%时，写入本地文件。写前根据#reduce将数据划分为多个partition，避免某reducer分配到大量数据。对每个partition数据sort，根据combiner()进行combine，减少写入磁盘的数据。多个磁盘文件时，合并，并通过sort、combine减少磁盘数据量，下一步网络传输数据量。最终得到一个已分区且排序的文件。reducer：通过jobtracker得到map输出位置，接受不同map的数据。若数据量小，放入mem，超阈值写磁盘，并不断sort、combine。
硬件故障：jobtracker单点，用主备。tasktracker故障，jobtracker将其从集合中移除，若为map，交给其他tasktracker重新执行所有map任务，若为reduce，仅执行未完成的reduce job。
job失败引发的故障：用户代码抛异常，任务jvm自动退出，向tasktracker父进程发错误消息，并写log。tasktracker的监听程序发现进程退出，将其标记为失败。若程序死循环、时间太长，tasktracker没有收到进度，也标记失败，并kill进程。将自己的任务计数器-1，以便再向jobtracker申请新任务。jobtracker通过heartbeat收到失败通知，重置状态，加入调度队列重新分配。若尝试4次仍不成功，整个job失败。

8.zk
高性能：全量内存。高可用：集群。可靠：基于事务的更新。适合读多写少。
场景：配置中心，数据发布订阅。分布式锁，CP模型，保持独占，控制时序。分布式队列，FIFO，等所有成员聚齐后开始执行。负载均衡。master选举。可提供顺序一致性（每个写请求分配全局唯一递增id）；原子性；单一视图；可靠性；实时性。
文件树结构：znode，全量内存，包括stat（版本、权限等）；data；children。最大1MB。可设置watch，有增删改向client仅异步发一次通知，且保证client先收到watch事件再看到状态改变，顺序一致性。
集群角色：leader：可读写，维护集群状态。follower：可读写，定期向leader汇报状态，参与”过半写成功“和选leader。observer：可读写，汇报状态，不能参与”过半写成功“和选leader，可用于提高读性能。client：tcp长连接到集群，建立session，通过hearbeat维护状态。
ZAB：启动/leader故障时，集群进入恢复模式，可读但不可写。vote选leader，其他follower从他同步状态，完成后进入消息广播模式。leader接收写请求，分配zxid，广播。每个follower有队列，收到消息写本地log，返回ack给leader。leader收到>n/2 ack后，广播commit，且自己也commit。


9.hbase
与hdfs对比：hdfs追加写，全部/分区扫描，hive性能高，最大30PB；hbase可随机写，随机读，小范围/全表扫描，hive性能低4-5x，稀疏列存储，最大1PB。
region有多个store，每个store保存一个CF，由多个memstore和多个storefile组成。
client：cache .META数据。zk：master高可用（保证只有1个master运行，异常时切换备master），监控regionserver，meta入口，集群配置。hmaster：为rs分配region，维护集群LB，维护meta。发现失效region，将其分配到其他rs（hlog、storefile都在hdfs，rs无状态）。rs：读写数据，管理region，存储到hdfs。region过大，hmaster要求rs split。合并storefile。
master为region server分配region，负载均衡，为失效的region server重新分配其上的region，管理用户对table的增删改查操作。region server：维护region，处理IO，切分过大的region。zk：选举，保证只有一个master，不单点；存储所有region的寻址入口；监控region server的上下线，通知master；存储hbase schema和table meta。
每个regionserver一个HLog。用户操作写入memstore，及HLog。HLog定期滚动出新文件，并删除已经持久化到storefile中的旧log。region server挂，master通过zk感知，先处理遗留的hlog：将不同region的log拆分，放到对应region目录，再重分配失效region，新regionserver加载region时，replay hlog到memstore，再flush到storefile。
容错：master挂，zk重新选。无master时，可读数据，但不能region切分、负载均衡等。region server：定期发心跳到zk，若一段时间无心跳，master重分配region。
定位region server：zk -> -ROOT- ->.META. -> 用户表。ROOT表只有一个region，zk记录其位置，包括.META.表所在的region列表。META表包含用户空间region列表，和region server地址。

多租户：不同业务的表隔离在不同namespace。权限存储在hbase:acl表。每台RS维护完整permission cache，AccessController可在master、regionserver、region等操作的hook中检查权限，不足抛AccessDeniedException。授权：client向acl region的RS发grant/revoke请求；RS将权限put/delete到acl表；AccessController将更新写入zk；通过zk watcher，通知master、其他RS更新permission cache。

hbase rowkey:

最长64KB，实际常10-100B。内部byte[]，按字典序排序。

ns：划分业务线。权限管理；region quota限制。表默认放default下，ns hbase放系统表。



读：与master无关

1.client从zk获取meta表放在哪个region server。rs1

2.client请求rs1，获取rowkey放在哪个region server。meta表：表：start rowkey-end rowkey -> rs。rs2

3.client请求rs2，先memstore，再storefile。若访问storefile，会放入block cache，下次可从cache返回（LRU）。
scan：1.scanapi：业务for(Result r: resultScanner)时，client先判断是否有cache，没有则发next请求给rs，获取100条数据/最大2M数据。返回后cache到本地。限制返回的数据量大小，避免一次请求过多使集群带宽不稳定；避免client OOM和timeout。缺：不同region的scan也不会并发（multiget可并发）；scan时client阻塞等待。适合OLTP少量扫描场景，且最好设置start、endkey、setFamilyMap。 2.TableScanMR：scan.setCache(500); scan.setCacheBlocks(false); 根据region分成多个sub-scan api，并行扫描原始表。适合OLAP离线大量扫描，时间基本等于最大region扫描时间。若有数据倾斜，可设置hbase.mapreduce.input.autobalance=true，将大region scan切分为多个小scan。3.snapshotscanMR：扫描原始表的snapshot resotre出来的hfile，在client直接打开region扫hdfs文件，不需要发给rs。减少对rs的影响，提高>2倍效率（网络传输小，避免rs瓶颈）。但都是一个region一个mapper，需提供其他策略细分粒度。


写：与master无关。写比读快，100台RS可支持100T/d写入。
1.client配置autoflush=true，同步发送到server，=false，先放本地buf，由AsyncProcessor异步发送到server，可能丢失。从zk获取meta表所在rs。rs1

2.client从rs1获取rowkey所在rs。rs2

3.client写rs2 hlog，memstore。delete是mark delete，compact实际删除。同一行数据放在同一region server，通过行锁保证行操作原子。checkAndPut/Delete/Increment/append内部也是通过行锁，加锁->get->比较->相等则put/del，否则返回失败。put/del时，先获取行锁，再获取write number，用于mvcc；写hlog，再写memstore，异步线程将hlog刷到hdfs。hlog可sync写，同步写入文件系统，不落盘（默认）。或async写，落盘。写入本地log即成功。释放行锁，共享锁。hlog结构：rowkey：region_seqid_writetime_tablename_clusterid，value：#kvs+list<kv of same row>


flush & compact：hbase-default.xml  由master触发，rs执行。

region server中所有region的所有mem store之和>40%堆内存；某region中所有memstore之和>128MB；某memstore超过1h没有flush，则该region所有memstore都flush。

flush可能产生小文件，通过compact合并后再存到hdfs。时间、size。

region split：等数据增长到一定程度自动split，会有热点写，浪费io。预估数据，预分区。一台rs，可有100个region。同一个表，一个region可放2-3个region。时机：ConstantSizeRegionSplitPolicy，某region最大store超过hbase.hregion.max.filesize时。没区分大小表，若阈值较大，小表不易切分。IncreasingToUpperBoundRegionSplitPolicy，自动调整阈值，可自适应大小表，但大集群时小表会有大量小region。SteppingSplitPolicy，与表的region个数有关，更友好。切分点：region中最大store的最大文件的中间block的首个rowkey。过程：原子事务，2PC。prepare：内存中初始化两个子region，生成两个HRegionInfo(tablename,regionname,startkey,endkey)，和transaction journal，记录进展。execute：rs修改zk /region-in-transition/parent_ragion_name:splitting。master检测到该节点，修改内存中region状态，可在页面模块看到。rs在hdfs 创建 /parent_region_name/.split/ 保存子信息。关闭parent region，flush，client请求得到NotServingRegionException。在hdfs创建 .split/a, b目录 和/a/column_family/reference_files，该引用文件记录splitkey和子是父的上半/下半部分。父region分裂后，将a、b拷到hdfs跟目录下。parent 修改.meta表后下线。开启a、b，修改.meta。rollback：状态机记录每个步骤的状态，根据切分状态清理垃圾。split无数据移动，major compaction时将父region数据移到子region目录，然后删除.meta表的父region。

优化：region server堆内存大小，70%，避免GC过频繁/过久。datanode打开文件数，4096。数据写入效率：compress。rpc监听数量：30。hfile split大小：10G。

snapshot：对文件系统、目录在某时刻的镜像。用于在线全量/增量备份；集群数据迁移。1.加锁拷贝，拷贝期间只读。2.snapshot：加锁，将memstore数据flush到hfile，建指针引用hfile，meta为snapshot。hfile落盘后不能写，当前表的所有文件建引用，新数据写入新文件。hbase通过2PC保证所有执行snapshot的rs原子操作。prepare：master在zk创建/acquired-snapshotname节点，写入snapshot table信息。所有rs监听到该节点，查看本机是否有该table。若有遍历该table所有region，执行snapshot，写入tmp目录。完成后在zk创建/acquired-snapshotname/nodeid。commit：所有rs完成后，master创建/reached-snapshotname，所有rs监听到该节点，将tmp目录移到最终目录，并创建/reached-snapshotname/nodeid。abort：所一定时间内/acquired-snapshotname下的节点个数不满足，master认为prepare超时，创建/abort-snapshotname，所有rs监听后清理tmp目录。

10.storm
流计算框架，内存计算，通过移动数据平均分配到机器资源来提高效率。高容错，通过ack使不丢消息。速度快，单节点可100w tuple/s。
对比flink：storm无状态；对事件窗口支持弱，缓存整个窗口数据，结束时一起计算；支持at most/least once；通过ack追踪消息，失败/timout重发。flink：有状态；支持窗口聚合，自动管理；可exactly once；通过checkpoint容错，保存数据流、算子状态，错误时可回滚。
topology：完整处理程序，是spout、bolt通过stream连接的有向无环图。spout：可靠（失败可重发）；不可靠。由多个task并行执行。
grouping策略：bolt间。shuffle：随机。fields：根据某字段。all：所有bolt task都复制一份，重复处理。global：整个stream都进入其中一个bolt。direct。local：若两bolt在同一worker进程，优先发本地，减少网络传输。
nimbus进程：通过thrift接口监听client提交的topology；根据worker资源情况，分配task，并记录到zk；通过thrift监听supervisor下载topo代码的请求，并提供下载；thrift监听UI对统计信息的读取，读zk。
supervisor进程：按需启动worker进程。定时从zk检查是否有新topo代码，删旧代码；根据nimbus分配的task启动worker，并监控。
worker进程：构造spout、bolt task，启动executor线程。从zk获取task，构造，交给executor线程执行；zk写heartbeat；维持传输队列，将tuple传输给其他worker。
zk：存储状态，nimbus和supervisor都是无状态的。

11.rpc
包括：数据格式；协议（data meta）；conn管理（连接池）；异步、超时；naming service、LB、组合访问。







