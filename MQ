《kafka》
8.集群参数
broker:log.dirs。可挂载到不同物理磁盘，提升读写性能，故障转移（1.1前任一磁盘挂掉会关闭整个broker，1.1后将坏磁盘数据自动转移到其他正常磁盘，broker正常）。zookeeper.connect。listeners：外部连接者通过什么协议访问指定host:port的kafka服务。auto.create.topic.enable:false。若producer启动时写错了topic，会自动创建。auto.leader.rebalance.enable=false。
topic：保留时间，retention.ms：默认7d。retention.bytes:最大磁盘。max.message.bytes：能接收的最大消息。
jvm：heap 6GB。gc：若cpu充足用cms，否则用吞吐量收集器。jdk8用g1，full gc少。
os：fd限制，ulimit -n。swap：较小值，若为0,物理内存耗尽时oom，os会随机选一个进程kill，若设为较小值，容易观测到broker性能急剧下降。提交时间/flush落盘时间：数据写入os page cache即可认为成功，根据LRU定期（5s）写入磁盘，但flush前的数据可能丢失，由于有多副本，可以容忍。

9.producer partition
负载均衡，高伸缩性，partition内部消息顺序性。
分区策略：partition.class，实现Partitioner接口，partition(),close()。轮询；key-ordering，按key.hashcode%#partitions；根据ip的geo-location。

10. producer compression
message：v1每条msg都有CRC校验，v2为msg set进行CRC校验。V1每条消息压缩，v2对整个msg set压缩，效率更高。
producer配置：compression.type，启用对应压缩算法，类型记录到msg set中。节省网络带宽和broker磁盘，在consumer解压缩。但若broker和producer压缩算法不同，broker会先解压缩，再用自己的压缩算法重新压缩。或broker发生消息格式转化，也会解压缩，且丧失0copy特性。

11.无消息丢失
已提交消息：若干broker成功收到并写入log。用producer.send(msg，callback)带回调的异步发送，acks=all，所有replica都收到消息才成功。retries自动重试设置较大值。unclean.leader.election.enable=false，当leader挂掉，落后太多的follower不能成为leader。replication.factor>=3。min.insync.replicas>1，消息至少写入几个replica。 enable.auto.commit=false


12.producer 拦截器
interceptor.classes：实现ProducerInterceptor接口，onSend()发送前调用,onAck()发送成功或失败调用，在callback前，与onsend不在同一线程，注意共享变量的线程安全问题。ConsumerInterceptor接口，onConsume()消息处理前,onCommit()提交offset后。

13.producer tcp
tcp多路复用请求：将多个数据流合并到底层单一物理连接。即在一条物理连接上创建多个虚拟连接，每个连接负责各自对应的数据流。
建立连接：new KafkaProducer()时，后台创建sender线程，建立与bootstrap.servers中配置的broker的连接。一般配3~4台，连接到任一broker后可得到整个集群的broker信息。再向某台broker发metadata请求。若发现与某些broker当前没有建立连接，则创建。send()时，若发现目标broker没有建立连接，创建。metadata.max.age.ms定期更新meta。
关闭连接：主动关闭producer.close()；被动由broker关闭，connections.max.idle.ms=9min，若9min内该conn没有数据，则broker端关闭。producer会产生close_wait连接，无法显式观测到连接中断。

14.producer 幂等 & 事务
默认at least once。
幂等producer：props.put("enable.idempotentce",true)。broker多保留一些字段，当收到有相同字段的消息时，自动去重。只能保证partition内去重，单session内去重，重启producer会失效。
多partition、多session去重：事务。read commited级别，可将多消息原子写入多个partition，且consumer只能看到commit成功的消息。设置enable.idempotentce，和producer transactional.id。producer.initTransaction(); producer.beginTransaction(); producer.send(msg); ... producer.commitTransaction();/abort。consumer端：read_commited。broker通过2pc，通过transactional coordinator完成。性能问题。

15.consumer group
可扩展、容错的consumer机制，有多个consumer实例，可多进程/多线程。#consumer=#partition。

16. _consumer_offsets topic
老版本consumer offste提交到zk。但zk不适合高频写。=》存到offset topic。消息格式：key：<consumer_groupid ,topic,partition>，value：offset，时间戳，用户自定义数据等。可用于删除过期offst等操作。其他消息：保存consumer group信息的消息（注册consumer group时）；删除group过期offset的消息，或删除group的消息（delete mark，当consumer group下所有consumer都停止了，且offst数据已被删除，kafka写入该消息，彻底删除该group的消息）。
创建：集群启动第一个consumer程序时。分区数：offsets.topic.num.partitions=50，replica=3。
删除过期offset：compaction。后台线程log cleaner，定期扫描log的所有消息，同一key的不同消息，取发送时间最新的，其他为过期，删除。

17.consumer group rebalance
一个group下所有consumer，通过coordinator，就如何消费partition达成共识。此过程中，所有consumer都不能消费。
coordinator：每个broker都有coordinator组件，为consumer group执行rebalance，提供offset管理，group member管理。consumer commit offset是通过coordinator所在broker提交。consumer启动时向coordinator broker发请求，进行注册、管理member等meta。consumer group如何确定coordinator所在broker？先根据groupid%50得到partitionid，找到该partition leader所在broker。
rebalance缺点：停止consumer；慢；不考虑局部性，效率低。Sticky assigner：尽可能保存之前分区方案。
rebalance时机：group member变化；topic变化；partition变化。=》后两者不可避免，如何避免member变化？启动一个group.id为当前group的consumer，发请求给coordinator，加入，rebalance。为了增加tps，不可避免。减少：consumer在session.timeout.ms=10s没有发送heartbeat给coordinator，发送间隔为heartbeat.interval.ms，一般timeout是interval>3倍。检查consumer是否因gc超时。


18.consumer offstet commit
记录下一条消息的offset。
enable.auto.commit=true，consumer后台自动提交，提交间隔：auto.commit.interval.ms,默认5s。在每次开始调用poll()时，先提交上次poll()的最新offset。可保证不丢，但可能重复：若两次commit间，发生rebalance。所有consumer从上次commit offset开始消费，则两次commit间的数据会重复。
手动提交：enable.auto.commit=false。且consumer.commitSync()，同步提交consumer.poll()得到的最新offset，阻塞直到broker返回结果。若有瞬时错误，会自动重试。consumer.commitAsync(offset，callbackhandler)异步commit，异常时不能重试。=》组合：先async提交避免阻塞，在consumer.close()前，sync commit保证offset正确。若一次poll很多消息，可在consume期间commit offset。

19.CommitFailedException
调用consumer.commitSync()时，1)若commit期间，consumer group开始rebalance，且将要commit的parition分配给其他consumer。2)consumer处理时间太长，耽误了poll()。=》增加max.poll.interval.ms，或减少每次poll的消息数max.poll.records，或缩短单条消息处理时间，或下游多线程加速消费（如何多线程commit？）。3)不同consumer group的group.id冲突




RabbitMQ：高并发、高吞吐、性能高；后台管理界面；集群化、高可用部署、消息高可靠；实践多、社区活跃。缺点：erlang开发，难改造。有broker的复杂路由：exchange：根据复杂的业务规则将msg路由到不同queue。queue：exchange向一个queue写msg，多个consumer消费该queue，但每条msg只能给一个consumer。pub/sub：每个consumer一个queue，绑定到exchange。若某consumer只消费某类数据，exchange可将带xx前缀的msg发到该queue。吞吐比kafka低一个数量级。适合不同业务系统要复杂消息路由的场景。eg.A发10条msg，3条给b，7条给c。
RocketMQ:高并发、高吞吐、分布式事务。
kafka：消息中间件的功能少，优化吞吐。适合高吞吐的日志采集、数据同步、实时计算，几十万/百万/s。结合storm、spark streaming等。有broker的暴力路由：生产消费模型用简单的数据流模型，producer向topic写数据，分成多个partition，消息发到某个partition。消费时，每条消息只发给consumer group中的一个consumer，即partition：consumer=*：1，形成queue。publish/subscribe模型：生产者的每条消息要让所有consumer消费，使每个consumer一个group，订阅相同topic。

引入原因：系统解耦；异步调用；流量削峰；实时计算
缺点：系统可用性降低；稳定性降低（丢数据、重复、下游宕机后消息堆积）；事务一致性。

1. kafka
如何实现每秒几十万的高并发写入？
页缓存+磁盘顺序写：每次收到数据，写入os page cache，再由os顺序写到disk。
0拷贝读：先看数据是否在os cache，若不在读磁盘后后放入os cache。再从os cache拷贝到kafka进程cache，再拷贝到socket cache，再发到网卡。多次拷贝，上下文切换。=》将fd拷贝到socket cache，将数据从os cache发到网卡。
如何保证数据不丢？
业务定义topic；topic可拆分为多个partition，分配到不同机器；每个partition多副本放在其他机器（1leader+多follower，主从复制，只写leader），防止宕机丢数据。
若数据写入leader，但还没同步到follower，leader宕机：每个partition至少有1个follower在isr列表；写入时，要求leader和至少一个follower写入成功，否则写入失败重试。
ISR：每个partition对应一个isr列表，包括leader+与leader同步的follower。若某follower没及时同步（如JVM GC等），out of sync，移出isr。
Producer ACK：0，只要prod发送成功就认为成功，即使没有在leader落盘,有可能丢。1,leader写入磁盘，不管其他follower有没有同步成功，就认为成功，默认。当leader挂掉，follower还没同步，可能丢。all，leader落盘，且ISR中的follower都同步成功才成功。若isr只有一个leader，没有follower仍可能丢。


2. 如何设计消息中间件
存储：内存 or 磁盘？先写内存做缓冲，每隔几秒刷入磁盘。磁盘文件拆分规则。磁盘文件meta：数据offset、id。数据量增大，分片，扩容。
某机器宕机后的数据丢失问题：多副本。
生产端ack：
消费端ack：手动。

3.RabbitMQ
需要配queue、message是否持久化：不保证完全不丢消息。接收到消息但还没持久化时，宕机则丢数据。=》事务消息（性能差）；轻量级的confirm。
消费者：与mq建立channel，根据自增的deliverytag ack。可批量ack。

2.MQ高可用
1) kv集群
封装mq client：连续10次retry都有异常报错、网络无法联通等，自动触发zk降级开关。
降级方案：redis队列替代queue。但不能写value过大的数据；不能往少数热点key持续写数据（kv集群通常用hash（key）分配到不同机器，热点key会导致某机器负载过大）=》根据消息量，在kv中划分上百个队列，对应上百个key。故障后对每个消息进行hash，均匀写入固定的kv队列。
下游mq client降级感知：
自动恢复：后台线程，每隔一段时间尝试给mq发消息看是否恢复。可发消息后自动关闭zk降级开关。向mq发消息，下游消费完kv消息后，切换到mq。
2) 异步转同步+限流+限制性丢弃流量
不写mq，直接调用流控集群的备用接口。若流量超过阈值，丢弃。丢弃策略：电商，仅选少量商家的全部丢弃，大部分全量保存。

3.KV高可用
临时扩容slave+内存分片存储+小时级数据粒度
报警后手动扩容n倍slave集群，流控集群按预设的hash算法分发数据。slave只存最近1h的数据。

4.实时计算链路
slave宕机，master感知，将计算任务重新分配到其他节点。master宕机，基于active-standby，自动切换主备。
